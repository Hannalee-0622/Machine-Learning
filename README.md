# Machine-Learning

이 리포지토리는 머신러닝의 핵심 개념과 알고리즘을 이론적으로 깊이 있게 탐구하고, Python 코드와 Jupyter Notebook을 통해 이러한 이론들을 실제로 구현해보는 것을 목표로 합니다. 각 디렉터리는 특정 머신러닝 이론 및 기법에 초점을 맞추고 있으며, 이론적 배경, 수학적 원리, 그리고 실제 코드 예제와 데이터셋을 포함하여 학습 효과를 극대화하고자 합니다.

---
## 디렉터리 구조
```
Machine-Learning/
├─ 01.FigurePlot/ # 그래프 출력 & 시각화
├─ 02.One-HotEncoding/ # 원-핫 인코딩 기법
├─ 03.LinerModel/ # 선형 모델(회귀·분류) 이론
├─ 04.LinerTraining/ # 선형 모델 학습(경사 하강법)
├─ 05.Multi_Input/ # 다중 입력(Multi-Input) 처리
├─ 06.LogisticRegression/ # 로지스틱 회귀 I (이론)
├─ 07.GradientDescent_Algorithm/ # 경사 하강법 알고리즘 심화
├─ 08.LogisticRegression/ # 로지스틱 회귀 II (응용)
├─ 09.SoftMax_Classifier/ # 소프트맥스 분류기
├─ 10.Multiclass_Classification/ # 다중 클래스 분류 (타이타닉 예제)
```
---

## 📖 목차

1. [데이터 시각화 (FigurePlot)](#1-데이터-시각화-figureplot)  
2. [원-핫 인코딩 (One-HotEncoding)](#2-원-핫-인코딩-one-hotencoding)  
3. [선형 모델 (LinerModel)](#3-선형-모델-linermodel)  
4. [학습 과정 (LinerTraining)](#4-학습-과정-linertraining)  
5. [다중 입력 (Multi_Input)](#5-다중-입력-multi_input)  
6. [로지스틱 회귀 I (LogisticRegression)](#6-로지스틱-회귀-i-logisticregression)  
7. [경사 하강법 (GradientDescent_Algorithm)](#7-경사-하강법-gradientdescent_algorithm)  
8. [로지스틱 회귀 II (08.LogisticRegression)](#8-로지스틱-회귀-ii-08logisticregression)  
9. [소프트맥스 분류기 (SoftMax_Classifier)](#9-소프트맥스-분류기-softmax_classifier)  
10. [다중 클래스 분류 (Multiclass_Classification)](#10-다중-클래스-분류-multiclass_classification)  
---

## 주요 학습 개념

이 리포지토리에서는 다음과 같은 머신러닝의 핵심 이론과 실제 적용 방법을 상세히 다룹니다:

### 01. FigurePlot (그래프 출력)
* **이론적 중요성**: 데이터 시각화는 머신러닝 프로젝트의 초기 단계에서 데이터의 분포, 패턴, 이상치 등을 파악하고, 모델 학습 후에는 모델의 성능, 결정 경계, 학습 과정 등을 분석하는 데 필수적입니다. 시각화를 통해 복잡한 데이터를 직관적으로 이해하고, 모델 개선을 위한 통찰력을 얻을 수 있습니다.
* **핵심 개념**: `numpy`를 사용한 데이터 생성 및 `matplotlib.pyplot`을 활용한 그래프 표현 방법을 익힙니다.
    * **수학 함수 시각화**: 기본적인 수학 함수(예: 사인 함수)를 시각화하여 함수의 주기성, 진폭 등의 특징을 관찰합니다.
    * **활성화 함수 시각화**: 머신러닝, 특히 딥러닝에서 중요한 역할을 하는 **시그모이드 함수(Sigmoid function)** $ \sigma(x) = \frac{1}{1 + e^{-x}} $를 시각화합니다. 시그모이드 함수는 S자 형태를 가지며, 입력값을 (0, 1) 사이의 값으로 변환하여 확률적 해석을 가능하게 하거나, 신경망에서 뉴런의 활성화 정도를 나타내는 데 사용됩니다. 이러한 비선형 함수는 모델이 복잡한 패턴을 학습할 수 있도록 돕습니다.
* **파일**:
    * `09-1_Sigmoid.py`: 시그모이드 함수의 정의와 그래프 출력.
    * `09_FigurePlot.py`: 사인 함수의 정의와 그래프 출력.

### 02. One-HotEncoding (원-핫 인코딩)
* **이론적 중요성**: 대부분의 머신러닝 알고리즘은 수치형 데이터를 입력으로 요구합니다. 따라서 '색상', '성별'과 같은 범주형 데이터를 모델이 처리할 수 있는 수치형 형태로 변환하는 과정이 필요합니다.
* **핵심 개념**: **원-핫 인코딩(One-Hot Encoding)**은 범주형 변수를 표현하는 대표적인 방법입니다. 각 범주를 고유한 이진 벡터로 변환합니다. 예를 들어, K개의 범주가 있다면 각 범주는 K차원의 벡터로 표현되며, 해당 범주에 해당하는 인덱스만 1이고 나머지는 0으로 채워집니다.
    * **장점**: 범주 간의 순서가 없거나(명목형 변수), 순서 정보를 모델에 잘못 전달할 위험을 피할 수 있습니다. 예를 들어, {'Red':0, 'Green':1, 'Blue':2}와 같이 단순 정수 인코딩을 하면 모델이 Green(1)이 Red(0)보다 크고 Blue(2)보다 작다는 인위적인 순서 관계를 학습할 수 있습니다. 원-핫 인코딩은 이러한 문제를 방지합니다.
    * **고려사항**: 범주의 수가 매우 많을 경우 데이터의 차원이 급격히 증가하여(차원의 저주), 계산 비용이 커지거나 모델 성능에 부정적인 영향을 줄 수 있습니다.
    * `tensorflow`의 `tf.one_hot` 함수를 사용하여 정수형 레이블을 원-핫 벡터로 변환하고, 이를 다중 클래스 분류 문제의 레이블로 사용하는 방법을 학습합니다.
* **파일**:
    * `10_One-HotEncoding.py`: `tf.one_hot`을 이용한 원-핫 인코딩 시연 및 소프트맥스 분류기 적용 예시.

### 03. LinerModel (선형 모델)
* **이론적 중요성**: 선형 모델은 입력 변수와 출력 변수 간의 관계를 직선(또는 초평면)으로 가정하는 가장 단순하면서도 강력한 모델 중 하나입니다. 복잡한 현상도 국소적으로는 선형 관계로 근사할 수 있으며, 많은 고급 알고리즘의 기초가 됩니다.
* **핵심 개념**:
    * **선형 가설(Linear Hypothesis)**: 입력 변수 $x$에 대한 예측값 $H(x)$ (또는 $\hat{y}$)를 $H(x) = Wx + b$ (단일 변수) 또는 $H(X) = XW + b$ (다변수)로 정의합니다. 여기서 $W$는 가중치(weights), $b$는 편향(bias)으로 모델이 학습해야 할 파라미터입니다.
    * **비용 함수(Cost Function) / 손실 함수(Loss Function)**: 모델의 예측값과 실제값 사이의 차이를 측정하는 함수입니다. 모델 학습의 목표는 이 비용 함수를 최소화하는 파라미터 $W, b$를 찾는 것입니다.
    * **평균 제곱 오차(Mean Squared Error, MSE)**: 회귀 문제에서 가장 널리 사용되는 비용 함수 중 하나로, 다음과 같이 정의됩니다: $J(W, b) = \frac{1}{m} \sum_{i=1}^{m} (H(x^{(i)}) - y^{(i)})^2$. MSE는 오차의 제곱에 비례하여 큰 오차에 더 큰 패널티를 부여하며, 미분 가능하고 볼록(convex)한 형태(선형 회귀의 경우)를 가져 경사 하강법 등을 통해 안정적으로 최적해를 찾을 수 있게 합니다.
    * 코드에서는 가중치 $W$ 값의 변화에 따른 MSE 값의 변화를 시각화하여, 비용 함수의 형태와 최소점을 직관적으로 보여줍니다.
* **파일**:
    * `11_LinerModel.py`: 단일 변수 선형 모델에서 가중치 $W$ 변화에 따른 MSE 비용 함수 시각화.

### 04. LinerTraining (선형 모델 학습)
* **이론적 중요성**: 모델을 정의했다면, 실제 데이터를 가장 잘 설명하는 최적의 파라미터를 찾아야 합니다. 이 과정을 '학습' 또는 '훈련'이라고 하며, **경사 하강법(Gradient Descent)**은 이러한 최적화 문제 해결에 널리 사용되는 알고리즘입니다.
* **핵심 개념**:
    * **경사 하강법(Gradient Descent)**: 비용 함수 $J(W,b)$를 최소화하는 $W, b$를 찾기 위한 반복적인 최적화 알고리즘입니다.
        1.  $W$와 $b$를 임의의 값으로 초기화합니다.
        2.  현재 $W, b$ 위치에서 비용 함수의 **그래디언트(gradient)**, 즉 각 파라미터에 대한 편미분 값($\frac{\partial J}{\partial W}$, $\frac{\partial J}{\partial b}$)을 계산합니다. 그래디언트는 함수 값이 가장 가파르게 증가하는 방향을 나타냅니다.
        3.  비용을 줄이기 위해 그래디언트의 반대 방향으로 파라미터를 업데이트합니다. 이때 **학습률(learning rate)** $\alpha$는 업데이트 폭(step size)을 결정합니다.
            * $W := W - \alpha \frac{\partial J}{\partial W}$
            * $b := b - \alpha \frac{\partial J}{\partial b}$
        4.  비용 함수 값이 충분히 작아지거나, 더 이상 변하지 않을 때까지 2-3단계를 반복합니다.
    * **학습률($\alpha$)**: 너무 작으면 수렴 속도가 매우 느리고, 너무 크면 최적점을 지나쳐 발산(diverge)할 수 있으므로 적절한 값 설정이 중요합니다.
    * 코드에서는 위 업데이트 규칙을 사용하여 $W$와 $b$를 반복적으로 갱신하며 비용 함수를 최소화하는 과정을 보여줍니다.
* **파일**:
    * `12_LinerTraining.py`: 단일 변수 선형 회귀 모델에 대한 경사 하강법 수동 구현 및 학습 과정 시각화.

### 05. Multi_Input (다중 입력)
* **이론적 중요성**: 실제 대부분의 문제는 여러 개의 입력 변수(특성, feature)를 가집니다. 다변수 선형 회귀는 이러한 다수의 특성을 고려하여 예측 모델을 구축하는 방법입니다.
* **핵심 개념**:
    * **다변수 선형 가설**: $n$개의 특성 $x_1, x_2, \ldots, x_n$이 있을 때, 가설은 $H(x_1, \ldots, x_n) = w_1x_1 + w_2x_2 + \ldots + w_nx_n + b$로 확장됩니다. 이를 행렬 형태로 표현하면 $H(X) = XW + b$가 됩니다. 여기서 $X$는 $m \times n$ 입력 행렬, $W$는 $n \times 1$ 가중치 벡터, $b$는 스칼라 편향입니다. 각 가중치 $w_j$는 해당 특성 $x_j$가 결과에 미치는 영향의 크기와 방향을 나타냅니다.
    * **비용 함수 및 최적화**: 다변수의 경우에도 비용 함수(주로 MSE)를 정의하고, 경사 하강법을 사용하여 이 비용을 최소화하는 가중치 벡터 $W$와 편향 $b$를 찾습니다. 각 가중치 $w_j$에 대해 편미분을 계산하여 업데이트합니다.
    * **`tf.GradientTape`**: TensorFlow에서 제공하는 자동 미분 기능입니다. 복잡한 모델이나 수동으로 그래디언트를 계산하기 어려운 경우, `GradientTape`를 사용하면 연산 과정을 기록하고 이를 바탕으로 자동으로 그래디언트를 계산해주어 구현의 편의성을 크게 높입니다.
* **파일**:
    * `13_Multi_input.py`: `tf.GradientTape`를 활용한 다변수 선형 회귀 모델 학습 및 사용자 입력 기반 예측 테스트.

### 06. LogisticRegression (로지스틱 회귀 - 이진 분류)
* **이론적 중요성**: 로지스틱 회귀는 결과가 두 개의 범주 중 하나인 경우(예: 예/아니오, 합격/불합격)를 예측하는 대표적인 이진 분류 알고리즘입니다. 선형 회귀의 출력을 확률 형태로 변환하여 분류 문제에 적용합니다.
* **핵심 개념**:
    * **선형 회귀의 한계**: 선형 회귀의 출력값($H(X)=XW+b$)은 $(-\infty, \infty)$ 범위를 가지므로, 이를 직접 확률(0~1 사이)로 해석하기 어렵습니다.
    * **시그모이드 함수(Sigmoid/Logistic Function)**: 이 문제를 해결하기 위해 선형 함수의 출력을 시그모이드 함수 $\sigma(z) = \frac{1}{1 + e^{-z}}$ (여기서 $z=XW+b$)에 통과시킵니다. 시그모이드 함수의 출력은 항상 (0, 1) 사이의 값을 가지므로 확률로 해석할 수 있습니다. 따라서 로지스틱 회귀의 가설은 $H(X) = \sigma(XW+b)$가 됩니다.
    * **결정 경계(Decision Boundary)**: $H(X)$가 특정 임계값(일반적으로 0.5)보다 크면 클래스 1, 작으면 클래스 0으로 예측합니다. $H(X)=0.5$가 되는 지점, 즉 $\sigma(XW+b)=0.5 \Rightarrow XW+b=0$이 되는 경계면을 결정 경계라고 합니다.
    * **비용 함수 - 교차 엔트로피(Cross-Entropy Loss / Log Loss)**: 로지스틱 회귀에서는 MSE 대신 교차 엔트로피 손실 함수를 사용합니다. 이는 모델이 잘못된 예측을 높은 확률로 했을 때 큰 패널티를 부여하고, 최대 가능도 추정(Maximum Likelihood Estimation) 원리로부터 유도될 수 있으며, 경사 하강법 적용 시 볼록한 형태를 유지하여 안정적인 학습을 가능하게 합니다.
        * $J(W, b) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)}\log(H(x^{(i)})) + (1-y^{(i)})\log(1-H(x^{(i)}))]$
    * 이 비용 함수를 경사 하강법으로 최소화하여 최적의 $W, b$를 찾습니다.
* **파일**:
    * `14_LogisticRegression.py`: 로지스틱 회귀 모델 구현, 교차 엔트로피 비용 함수 사용, 학습 및 시각화 (비용, 정확도, 결정 경계).
    * `SigmoidPlot.py`: 시그모이드 함수 자체의 형태와 특성 시각화.

### 07. GradientDescent_Algorithm (경사 하강법 알고리즘)
* **이론적 중요성**: 경사 하강법은 다양한 머신러닝 모델(선형 회귀, 로지스틱 회귀, 신경망 등)의 파라미터를 최적화하는 데 사용되는 핵심 알고리즘입니다. 비용 함수의 최소값을 찾아가는 과정을 이해하는 것은 모델 학습 원리를 파악하는 데 매우 중요합니다.
* **핵심 개념**:
    * **배치 경사 하강법(Batch Gradient Descent)**: 코드에서 구현된 방식은 전체 학습 데이터셋($m$개의 샘플)을 사용하여 한 번의 파라미터 업데이트마다 그래디언트를 계산합니다. 이는 안정적인 수렴 경로를 제공하지만, 데이터셋이 매우 클 경우 계산 비용이 많이 들 수 있습니다.
    * **수렴(Convergence)**: 반복적인 업데이트를 통해 비용 함수 값이 점차 감소하여 특정 값으로 안정화되는 현상입니다.
    * **학습률($\alpha$)의 중요성**:
        * **너무 작은 경우**: 수렴까지 많은 반복이 필요하여 학습 시간이 길어집니다.
        * **너무 큰 경우**: 최적점을 지나쳐 진동하거나 발산(비용 함수 값이 증가)할 수 있습니다.
        * 적절한 학습률을 찾는 것은 실험과 경험을 통해 이루어지며, 학습률 스케줄링(학습 진행에 따라 학습률을 조절)과 같은 기법도 사용됩니다.
    * **비용 함수 표면(Cost Function Surface)**: 파라미터 공간에서 비용 함수의 값들을 시각화하면 (예: 3D 플롯), 경사 하강법은 이 표면을 따라 가장 낮은 지점으로 이동하는 경로로 이해할 수 있습니다.
    * 이 섹션의 코드는 `advertising.csv` 데이터셋을 이용한 다변수 선형 회귀에서 그래디언트를 수동으로 계산하고, 학습률 변화에 따른 학습 과정을 비교하며 경사 하강법의 동작 원리를 심도 있게 보여줍니다.
* **파일**:
    * `15_GradientDescent_Algorithm.ipynb`: 수동 경사 하강법 구현, 광고 데이터셋 적용, 다양한 학습률 비교, 비용 함수 변화 및 3D 시각화.

### 08. LogisticRegression (로지스틱 회귀 응용)
* **이론적 중요성**: 앞서 학습한 로지스틱 회귀의 이론적 원리(시그모이드 함수, 교차 엔트로피 비용 함수, 경사 하강법을 이용한 최적화)가 실제 의료 데이터셋(`data-03-diabetes.csv`)에 어떻게 적용되어 유의미한 예측 모델을 구축하는지 보여줍니다.
* **핵심 개념**:
    * **특성(Features)**: 당뇨병 발병과 관련된 여러 의학적 측정치(예: 임신 횟수, 혈당 수치 등 8개)를 입력 특성으로 사용합니다.
    * **타겟 변수(Target Variable)**: 당뇨병 발병 여부(0 또는 1)를 예측합니다.
    * **모델 학습 및 평가**: `tf.GradientTape`를 사용하여 모델을 학습시키고, 학습 과정 동안 비용 함수 값과 정확도(accuracy)가 어떻게 변화하는지 관찰합니다. 이를 통해 모델이 얼마나 잘 학습되고 있는지, 과적합 또는 과소적합의 징후는 없는지 등을 판단할 수 있습니다.
    * **정확도(Accuracy)**: 전체 예측 중 올바르게 예측한 비율 ($ \frac{TP+TN}{TP+TN+FP+FN} $). 이진 분류 문제에서 가장 직관적인 성능 지표 중 하나이지만, 데이터 불균형이 심할 경우 해석에 주의해야 합니다.
* **파일**:
    * `16_LogisticRegression.ipynb`: 당뇨병 데이터셋을 활용한 로지스틱 회귀 모델 구축, 학습 과정(비용, 정확도) 시각화 및 모델 테스트.

### 09. SoftMax_Classfier (소프트맥스 분류기)
* **이론적 중요성**: 로지스틱 회귀가 이진 분류에 사용되는 반면, **소프트맥스 회귀(Softmax Regression)**는 세 개 이상의 클래스로 분류하는 다중 클래스 분류(Multiclass Classification) 문제에 사용됩니다. 각 클래스에 속할 확률을 계산하여 가장 확률이 높은 클래스로 예측합니다.
* **핵심 개념**:
    * **선형 점수(Logits)**: 각 클래스 $k$에 대해 선형 모델 $z_k = XW_k + b_k$를 계산합니다. 이를 로짓 또는 점수라고 합니다.
    * **소프트맥스 함수(Softmax Function)**: 이 로짓들을 확률 분포로 변환하는 함수입니다. 클래스 $j$에 속할 확률 $P(y=j|X)$는 다음과 같이 계산됩니다:
        * $P(y=j | X) = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}$ (여기서 $K$는 전체 클래스 수)
        * 소프트맥스 함수의 출력은 각 클래스에 대한 확률이며, 모든 클래스에 대한 확률의 합은 항상 1이 됩니다.
    * **원-핫 인코딩된 레이블**: 실제 레이블(정답)은 일반적으로 원-핫 인코딩 형태로 표현되어야 비용 함수 계산이 용이합니다. (예: 클래스 2 -> `[0, 0, 1, 0, ...]`)
    * **비용 함수 - 다중 클래스 교차 엔트로피(Categorical Cross-Entropy)**: 소프트맥스 회귀의 비용 함수로는 다중 클래스 교차 엔트로피가 사용됩니다.
        * $J(W, b) = -\frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K} y_k^{(i)} \log(H_k(x^{(i)}))$ (여기서 $y_k^{(i)}$는 $i$번째 샘플이 클래스 $k$에 속하면 1, 아니면 0이고 $H_k(x^{(i)})$는 모델이 예측한 $i$번째 샘플이 클래스 $k$에 속할 확률)
    * `data-04-zoo.csv` (동물 7종 분류) 데이터셋을 사용하여 소프트맥스 분류기를 구현하고 학습 과정을 살펴봅니다.
* **파일**:
    * `17_SoftMax_Classfier_.ipynb`: 동물 데이터셋을 사용한 소프트맥스 분류기 구현, 원-핫 인코딩 적용, 학습 및 성능(비용, 정확도) 시각화.

### 10. Multiclass_Classification (다중 분류 심화 - 타이타닉 생존자 예측)
* **이론적 중요성**: 실제 머신러닝 프로젝트는 단순히 모델을 선택하고 학습시키는 것 이상의 과정을 포함합니다. 이 섹션에서는 타이타닉 생존자 예측(이진 분류 문제이지만, 다중 분류 파이프라인 구축의 좋은 예시)을 통해 데이터 전처리부터 모델 학습, 교차 검증, 다양한 성능 평가 지표를 사용한 모델 비교 및 분석까지의 전체 워크플로우를 보여줍니다.
* **핵심 개념**:
    * **데이터 전처리 파이프라인 (`Pipeline`, `ColumnTransformer`)**:
        * **결측치 처리 (`SimpleImputer`)**: 모델 학습에 방해가 되는 결측치를 적절한 값(수치형: 중앙값, 범주형: 최빈값 등)으로 대체합니다.
        * **특성 스케일링 (`StandardScaler`)**: 수치형 특성들의 값 범위를 표준 정규 분포(평균 0, 표준편차 1)로 조정합니다. 이는 경사 하강법 기반 알고리즘의 수렴 속도를 높이고, 거리 기반 알고리즘(예: SVM, k-NN)의 성능을 향상시키는 데 도움이 됩니다.
        * **범주형 데이터 인코딩 (`OrdinalEncoder`, `OneHotEncoder`)**: 범주형 데이터를 모델이 처리 가능한 수치형으로 변환합니다. 특성의 순서 정보 유무에 따라 적절한 인코더를 선택합니다. `ColumnTransformer`는 이러한 다양한 변환을 각기 다른 특성 열에 선택적으로 적용할 수 있게 해줍니다.
    * **모델 선택 및 학습**:
        * **`RandomForestClassifier`**: 여러 개의 결정 트리를 사용하는 **앙상블(Ensemble)** 기법 중 하나인 **배깅(Bagging)**에 기반한 모델입니다. 개별 트리의 단점을 보완하고 과적합을 줄이며 일반화 성능을 높이는 경향이 있습니다.
        * **`SVC` (Support Vector Classifier)**: 두 클래스 간의 **마진(margin)**을 최대화하는 **결정 경계(hyperplane)**를 찾는 것을 목표로 하는 강력한 분류기입니다. 커널 트릭을 사용하여 비선형 데이터도 효과적으로 분류할 수 있습니다.
    * **모델 평가 및 비교**:
        * **교차 검증 (`cross_val_score`)**: 데이터를 여러 개의 폴드(fold)로 나누어 일부는 학습에, 일부는 검증에 사용하는 것을 반복하여 모델 성능을 보다 안정적으로 평가합니다. 이는 단일 학습/테스트 분할보다 일반화 성능에 대한 신뢰도를 높입니다.
        * **정밀도(Precision)와 재현율(Recall)**:
            * 정밀도 = $\frac{TP}{TP+FP}$ (모델이 Positive로 예측한 것 중 실제 Positive인 비율) - 모델 예측의 정확성.
            * 재현율 = $\frac{TP}{TP+FN}$ (실제 Positive 중 모델이 Positive로 예측한 비율) - 실제 Positive를 얼마나 잘 찾아내는지.
            * 정밀도-재현율 곡선(Precision-Recall Curve)은 임계값 변화에 따른 두 지표의 관계를 보여주며, 특히 불균형 데이터셋에서 유용합니다.
        * **ROC 곡선(Receiver Operating Characteristic Curve)과 AUC(Area Under the ROC Curve)**:
            * ROC 곡선은 임계값 변화에 따른 재현율(TPR, True Positive Rate)과 위양성률(FPR, False Positive Rate = $\frac{FP}{FP+TN}$)의 관계를 나타냅니다.
            * AUC는 ROC 곡선 아래 면적으로, 1에 가까울수록 모델의 분류 성능이 우수함을 의미합니다. 이는 모델이 양성 클래스와 음성 클래스를 얼마나 잘 구별하는지를 나타내는 종합적인 지표입니다.
* **파일**:
    * `다중 분류.ipynb`: 타이타닉 데이터셋을 이용한 데이터 전처리, `RandomForestClassifier` 및 `SVC` 학습, 교차 검증, 정밀도-재현율 곡선, ROC-AUC 분석 등 전체 머신러닝 파이프라인 시연.

---

## 설치 & 실행
```bash
git clone https://github.com/YourUser/Machine-Learning.git
cd Machine-Learning
python -m venv venv
source venv/bin/activate      # macOS/Linux
venv\Scripts\activate.bat     # Windows
jupyter notebook
```
---

**참고:** 각 폴더에는 해당 이론을 바탕으로 구현된 Python 코드와 이론 설명이 포함되어 있습니다. 자세한 내용은 각 폴더의 소스 코드를 참고해주세요.

---

## License

This project is licensed under the **MIT License**.
